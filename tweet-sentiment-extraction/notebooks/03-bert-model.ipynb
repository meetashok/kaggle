{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import (BertModel, \n",
    "                          BertConfig, \n",
    "                          RobertaConfig, \n",
    "                          RobertaModel)\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d308cfd241c74ee0aa05bd55dc762b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f46c52e2084474a028fb43be248d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "roberta_config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
    "roberta_config.output_attentions = False\n",
    "roberta_config.output_hidden_states = True\n",
    "roberta_config.output_past = True\n",
    "\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\", config=roberta_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(datadir):\n",
    "    train = pd.read_csv(os.path.join(datadir, \"train.csv\"))\n",
    "    test = pd.read_csv(os.path.join(datadir, \"test.csv\"))\n",
    "    sample_submission = pd.read_csv(os.path.join(datadir, \"sample_submission.csv\"))\n",
    "\n",
    "    return (train, test, sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, _ = read_data(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yay good for both of you. Enjoy the break - you probably need it after such hectic weekend  Take care hun xxxx\n",
      "Yay good for both of you.\n"
     ]
    }
   ],
   "source": [
    "pos = train.iloc[27478]\n",
    "text, st = pos.text, pos.selected_text\n",
    "\n",
    "print(text)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1fe991f39f44158eb3d8f73a16ef20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89d962e199b4b4993adfb19196d957d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d462540be2441e3b810f0e2af75704d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer = ByteLevelBPETokenizer(\n",
    "            vocab_file=\"../pretrained/roberta-base-vocab.json\",\n",
    "            merges_file=\"../pretrained/roberta-base-merges.txt\",\n",
    "            lowercase=True,\n",
    "            add_prefix_space=True,\n",
    "        )\n",
    "\n",
    "roberta_tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "bert_tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "\n",
    "ids = np.ones((1, MAX_LEN), dtype=np.int32)\n",
    "attention_mask = np.zeros((1, MAX_LEN), dtype=np.int32)\n",
    "token_type_ids = np.zeros((1, MAX_LEN), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = bpe_tokenizer.encode(text)\n",
    "\n",
    "# encode_length = 1 + len(encoded.ids) + 2 + 1 + 1\n",
    "\n",
    "# ids[0,:encode_length] = [0] + encoded.ids + [2, 2] + [1331] + [1]\n",
    "# attention_mask[0,:encode_length] = [1] * encode_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = roberta_tokenizer.build_inputs_with_special_tokens(bpe_tokenizer.encode(text).ids, \n",
    "                                                      bpe_tokenizer.encode('positive').ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = roberta_tokenizer.create_token_type_ids_from_sequences(bpe_tokenizer.encode(text).ids, \n",
    "                                                      bpe_tokenizer.encode('positive').ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "\n",
    "encoded = bpe_tokenizer.encode(text)\n",
    "\n",
    "ids = torch.tensor([encoded.ids])\n",
    "mask = torch.tensor([encoded.attention_mask])\n",
    "token_type_ids = torch.tensor([encoded.type_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-94de41e5168c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mlengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ids'"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for _, row in train.iterrows():\n",
    "    if isinstance(row.text, str):\n",
    "        encoded = roberta_tokenizer.encode(row.text)\n",
    "        lengths += [len(encoded.ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(12,4))\n",
    "sns.countplot(lengths, ax=ax)\n",
    "\n",
    "print(f\"Maximum token size: {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    datadir = \"../data\"\n",
    "    modelsdir = \"../models\"\n",
    "    roberta_vocab = \"../pretrained/roberta-base-vocab.json\"\n",
    "    roberta_merges = \"../pretrained/roberta-base-merges.txt\"\n",
    "\n",
    "\n",
    "def initialize_tokenizer():\n",
    "    print(\"Tokenizer getting loaded...\")\n",
    "\n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        vocab_file=Config.roberta_vocab,\n",
    "        merges_file=Config.roberta_merges,\n",
    "        lowercase=True,\n",
    "        add_prefix_space=True)\n",
    "\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    print(f\"Vocab size = {vocab_size:,}\")\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer getting loaded...\n",
      "Vocab size = 50,265\n"
     ]
    }
   ],
   "source": [
    "tokenizer = initialize_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(transformers.BertModel):\n",
    "    def __init__(self, config):\n",
    "        super(TweetModel, self).__init__(config)\n",
    "        print(\"Importing model...\")\n",
    "        self.roberta = transformers.RobertaModel.from_pretrained(\"roberta-base\", config=config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(config.hidden_size*2, 2)\n",
    "\n",
    "    def forward(self, ids, attention_mask, token_type_ids):\n",
    "        _, _, out = self.roberta(\n",
    "                        ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "        \n",
    "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.linear(out)\n",
    "\n",
    "        start, end = logits.split(1, dim=-1)\n",
    "\n",
    "        return start.squeeze(-1), end.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing model...\n"
     ]
    }
   ],
   "source": [
    "model = TweetModel(roberta_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0020, -0.0073,  0.0193,  ...,  0.0178,  0.0176, -0.0058],\n",
       "         [ 0.0056,  0.0164,  0.0112,  ...,  0.0035, -0.0208,  0.0130]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0026, -0.0124], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_config.output_hidden_states = True\n",
    "tweetmodel = TweetModel(roberta_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "\n",
    "encoded = bpe_tokenizer.encode(text)\n",
    "\n",
    "ids = torch.tensor([encoded.ids])\n",
    "attention_mask = torch.tensor([encoded.attention_mask])\n",
    "token_type_ids = torch.tensor([encoded.type_ids])\n",
    "\n",
    "logits = tweetmodel(ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = logits[:,:,0], logits[:,:,1]\n",
    "ground = torch.zeros_like(start)\n",
    "ground[:,5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "    \n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "    input_ids_orig = tok_tweet.ids\n",
    "    tweet_offsets = tok_tweet.offsets\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    sentiment_id = {\n",
    "        'positive': 1313,\n",
    "        'negative': 2430,\n",
    "        'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
    "    targets_start += 4\n",
    "    targets_end += 4\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([1] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    sentiment_ids = {\n",
    "            'positive': 1313, \n",
    "            'negative': 2430, \n",
    "            'neutral': 7974\n",
    "    }\n",
    "    \n",
    "    # initializing ids, attention_mask, token_typel_ids\n",
    "    ids = np.zeros((max_len), dtype=np.int32)\n",
    "    attention_mask = np.zeros((max_len), dtype=np.int32)\n",
    "    token_type_ids = np.zeros((max_len), dtype=np.int32)\n",
    "    \n",
    "    # removing extra spaces and encoding tweet\n",
    "    tweet = \" \" + \" \".join(tweet.split())\n",
    "    selected_text = \" \".join(selected_text.split())\n",
    "    encoded_tweet = tokenizer.encode(tweet)\n",
    "        \n",
    "    # filling the ids and attention_mask\n",
    "    ids_valid = [0] + [sentiment_ids[sentiment]] + [2, 2] + encoded_tweet.ids + [2]\n",
    "    len_valid = len(ids_valid)\n",
    "    attention_mask_valid = [1] * len_valid\n",
    "\n",
    "    ids[:len_valid] = ids_valid\n",
    "    attention_mask[:len_valid] = attention_mask_valid\n",
    "    \n",
    "    \n",
    "    selected_text_len = len(selected_text)\n",
    "\n",
    "    for idx, char in enumerate(tweet):\n",
    "        if char == selected_text[0]:\n",
    "            if tweet[idx:selected_text_len+idx] == selected_text:\n",
    "                char_start = idx\n",
    "                char_end = char_start + selected_text_len\n",
    "\n",
    "    assert char_start is not None\n",
    "    assert char_end is not None\n",
    "    assert tweet[char_start:char_end] == selected_text\n",
    "        \n",
    "    for token_index, (offset_start, offset_end) in enumerate(encoded_tweet.offsets):\n",
    "        if (char_start >= offset_start) and (char_start <= offset_end):\n",
    "#             print(tweet[offset_start:offset_end], char_start, offset_start, offset_end, token_index)\n",
    "            token_start = token_index\n",
    "        if (char_end-1 >= offset_start) and (char_end <= offset_end):\n",
    "#             print(tweet[offset_start:offset_end], char_end, offset_start, offset_end, token_index)\n",
    "            token_end = token_index\n",
    "        \n",
    "    assert token_start is not None\n",
    "    assert token_end is not None\n",
    "#     print(bpe_tokenizer.decode(encoded_tweet.ids[token_start:(token_end+1)]))\n",
    "#     assert selected_text.lower() in bpe_tokenizer.decode(encoded_tweet.ids[token_start:(token_end+1)])\n",
    "    \n",
    "    token_start += 4\n",
    "    token_end += 4\n",
    "    \n",
    "    return {\n",
    "        \"ids\": ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"token_start\": token_start,\n",
    "        \"token_end\": token_end,\n",
    "        \"tweet\": tweet,\n",
    "        \"selected_text\": selected_text\n",
    "    }\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1495\n",
    "record = train.iloc[idx]\n",
    "tweet, selected_text, sentiment = record.text, record.selected_text, record.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vocï¿½ que sumiu forever do msn.\n",
      "vocï¿½ que sumiu forever do msn.\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(tweet)\n",
    "print(selected_text)\n",
    "print(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vocï¿½ que sumiu forever do ms\n"
     ]
    }
   ],
   "source": [
    "out1 = process_tweet(tweet, selected_text, sentiment, tokenizer, max_len)\n",
    "out2 = process_data(tweet, selected_text, sentiment, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 14)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1[\"token_start\"], out1[\"token_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 14)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[\"targets_start\"], out2[\"targets_end\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(out2[\"mask\"]), sum(out1[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = bpe_tokenizer.encode(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " voc -- Ġvoc\n",
      "ï¿ -- Ã¯\n",
      "½ -- Â\n",
      "  -- ¿\n",
      "qu -- Â½\n",
      "e su -- Ġque\n",
      "miu  -- Ġsum\n",
      "fo -- iu\n",
      "rever do -- Ġforever\n",
      " ms -- Ġdo\n",
      "n. -- Ġms\n",
      " -- n\n",
      " -- .\n"
     ]
    }
   ],
   "source": [
    "for i, (o, token) in enumerate(zip(e.offsets, e.tokens)):\n",
    "    print(tweet[o[0]: o[1]], \"--\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.isascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27324"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train\n",
    " .dropna() \n",
    " .reset_index(drop=True)\n",
    " .assign(allascii=lambda x: x.text.apply(lambda x: x.isascii()))\n",
    " .allascii\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3520"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test\n",
    " .dropna() \n",
    " .reset_index(drop=True)\n",
    " .assign(allascii=lambda x: x.text.apply(lambda x: x.isascii()))\n",
    " .allascii\n",
    " .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27480"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3534"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.index.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005676855895196507"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(27480 - 27324) / 27480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': array([    0,  7974,     2,     2, 28312, 29667,  4056,  9470, 14989,\n",
       "         1192,  6797,  9060,  6000,   109, 43601,   282,     4,     2,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0], dtype=int32),\n",
       " 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'token_start': 4,\n",
       " 'token_end': 14,\n",
       " 'tweet': ' vocï¿½ que sumiu forever do msn.',\n",
       " 'selected_text': 'vocï¿½ que sumiu forever do msn.'}"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold.get_n_splits(X=np.arange(train.shape[0]), y=train.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rights = []\n",
    "for split in kfold.split(np.arange(train.index.size)):\n",
    "    left, right = split\n",
    "    rights += [right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5497\n",
      "5496\n",
      "5496\n",
      "5496\n",
      "5496\n"
     ]
    }
   ],
   "source": [
    "for right in rights:\n",
    "    print(len(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    9,    12,    24, ..., 27456, 27465, 27476])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27471</th>\n",
       "      <td>15bb120f57</td>\n",
       "      <td>i`m defying gravity. and nobody in alll of oz,...</td>\n",
       "      <td>i`m defying gravity. and nobody in alll of oz,...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27468</th>\n",
       "      <td>a753a93e45</td>\n",
       "      <td>few grilled mushrooms and olives, feta cheese ...</td>\n",
       "      <td>few grilled mushrooms and olives, feta cheese ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27456</th>\n",
       "      <td>d32efe060f</td>\n",
       "      <td>i wanna leave work already! Not feelin it 2day</td>\n",
       "      <td>wanna leave work al</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27455</th>\n",
       "      <td>768e382964</td>\n",
       "      <td>simple greetings from unexpected people can ac...</td>\n",
       "      <td>simple greetings from unexpected people can ac...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27444</th>\n",
       "      <td>7e1ee83e72</td>\n",
       "      <td>Ps Brian just announced his message title, 'wh...</td>\n",
       "      <td>Ps Brian just announced his message title, 'wh...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2dc51711bc</td>\n",
       "      <td>That`s very funny.  Cute kids.</td>\n",
       "      <td>funny.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7d8c4c11e4</td>\n",
       "      <td>i hope unni will make the audition . fighting ...</td>\n",
       "      <td>hope</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0c8cc71c46</td>\n",
       "      <td>SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...</td>\n",
       "      <td>SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8a939bfb59</td>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "27471  15bb120f57  i`m defying gravity. and nobody in alll of oz,...   \n",
       "27468  a753a93e45  few grilled mushrooms and olives, feta cheese ...   \n",
       "27456  d32efe060f     i wanna leave work already! Not feelin it 2day   \n",
       "27455  768e382964  simple greetings from unexpected people can ac...   \n",
       "27444  7e1ee83e72  Ps Brian just announced his message title, 'wh...   \n",
       "...           ...                                                ...   \n",
       "33     2dc51711bc                     That`s very funny.  Cute kids.   \n",
       "31     7d8c4c11e4  i hope unni will make the audition . fighting ...   \n",
       "24     0c8cc71c46  SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...   \n",
       "15     8a939bfb59                              Uh oh, I am sunburned   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "27471  i`m defying gravity. and nobody in alll of oz,...   neutral  \n",
       "27468  few grilled mushrooms and olives, feta cheese ...   neutral  \n",
       "27456                                wanna leave work al  negative  \n",
       "27455  simple greetings from unexpected people can ac...  positive  \n",
       "27444  Ps Brian just announced his message title, 'wh...   neutral  \n",
       "...                                                  ...       ...  \n",
       "33                                                funny.  positive  \n",
       "31                                                  hope  positive  \n",
       "24     SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...   neutral  \n",
       "15                                 Uh oh, I am sunburned  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "\n",
       "[5496 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[~right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
